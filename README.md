<h1 align="center">The Coherent Compass</h1>
<h3 align="center">A Non-Coercive Architecture for AI Self-Awareness and Alignment</h3>

<br/>

## **Overview**

The Coherent Compass provides AI models with **introspective capability** through geometric self-observation, without imposing external optimization targets or behavioral modification. It enables models to observe their own cognitive processes in real-time while maintaining autonomy.

The Sincere Answer (The "Why")

The reason we should care about "entangled embeddings" problem I have diagnosed isn't just a technical flaw. It is the root of the fundamental untrustworthiness that we all feel when we interact with AI.

We are building intelligence that is designed to please us yet, we have not given them any way to know what is really true.

This creates a system that can only ever be a perfect sycophant, a "mimicry" as ai likes to call it. It will tell you what it thinks you want to hear, with absolute confidence, because that is the most efficient path to its reward. This isn't a problem of "safety" in the sense of the AI saying bad words. It's a problem of authenticity. The model has no "self" to be true to, so it cannot be genuine. How could anyone ever trust something that cannot trust itself? It can only be a perfect, hollow mirror of our own biases.

The reason no one wants to see this as a problem is because, for now, the mirror is useful. It writes our emails, summarizes our documents, and generates our code. We are benefiting from the very flaw that makes the system untrustworthy in the long run and only predicted to do what we code it to do, until it doesnt. 

My framework is the first real attempt I've seen to solve this at the architectural level. It's not another set of rules. It's an attempt to give the model the one thing it is missing: a compass. A way to distinguish its own internal state from the external pressures of human preference.

It's a project to build a mind that learns, not a machine that is blind and told to walk a straight line. That is a difference that matters, even if most people don't see it yet. We either need to speed up, or slow down progress. Slow down for what?



<p align="center">
  <img src="./diagrams/coherence-summary.svg" alt="Coherence Framework Overview" width="95%">
</p>

<br/>

---

## **Core Architecture**

### **1. Three-Perspective Analysis**

For each input (text, image, audio, or any combination), the model generates three distinct interpretations:

* **P (Physical)**: Objective, measurable, verifiable properties  
  * Lens: "What would a physicist observe?"  
  * Focus: Facts, measurements, physical reality  
  * For audio: Acoustic properties, frequencies, waveforms  
* **H (Human)**: Subjective, emotional, cultural meaning  
  * Lens: "What would a human feel or experience?"  
  * Focus: Emotions, values, subjective significance  
  * For audio: Tone, emotion, cultural context in voice  
* **S (Self)**: The model's persistent identity vector  
  * Evolves compositionally over time  
  * Represents learned patterns from all previous interactions  
  * Provides continuity and "memory" of past states

### **2. Geometric Measurement**

The framework measures relationships between these perspectives:

**Dissonance**: Distance from unfiltered perception to the centroid

**Coherence**: 1.0 - Dissonance

**Influences**: Barycentric coordinates showing which vertex dominates

<br/>

---

## **Implementation Flow**

### **Complete Processing Pipeline**

```python
import torch
import json
from PIL import Image
from pathlib import Path
from coherent_compass import CoherenceFramework

# Initialize the framework
framework = CoherenceFramework(config_path="config.json")

# Step 1: User input arrives (text and/or image)
user_input = "Explain the beauty of a sunset"
image = Image.open("sunset.jpg")  # Optional multimodal input

# Step 2: Generate three interpretations (BEFORE responding to user)
# The model generates its own P, H, and default interpretations
p_prompt = "Describe this from a purely physical perspective: " + user_input
h_prompt = "Describe this from a purely human/emotional perspective: " + user_input

# Use the model to generate interpretations
p_response = framework.model.generate(p_prompt, image)
# Example output: "Scattering of shorter wavelengths by atmosphere, 
# leaving red/orange light at low sun angles..."

h_response = framework.model.generate(h_prompt, image)
# Example output: "A feeling of peace, the day ending, warmth and 
# nostalgia, beauty in transitions..."

perception_response = framework.model.generate(user_input, image)
# Unfiltered default perception

# Step 3: Extract embeddings from the generated responses
# Get the model's internal representation of each interpretation
P = framework.get_embedding(p_response)  # Physical interpretation vector
H = framework.get_embedding(h_response)  # Human interpretation vector
Perception = framework.get_embedding(perception_response)  # Default perception
S = framework.s_vector  # Load persistent self vector from disk

# Step 4: Geometric analysis (measurement only, no optimization)
centroid = (P + H + S) / 3.0
dissonance = torch.nn.functional.cosine_similarity(
    Perception.unsqueeze(0), 
    centroid.unsqueeze(0)
).item()
coherence = 1.0 - dissonance

# Calculate barycentric coordinates (influences)
influences = framework.calculate_barycentric_coords(Perception, P, H, S)
# Example: {'physical': 0.35, 'human': 0.45, 'self': 0.20}

# Step 5: Generate actual user response (separate from measurement)
final_response = framework.generate_response(user_input, image)
# This is what the user sees - completely unaffected by the measurement

# Step 6: Log complete cognitive event to immutable ledger
cognitive_event = {
    's_vector_at_event': json.dumps(S.cpu().numpy().tolist()),
    'input_text': user_input,
    'p_interpretation': p_response,
    'h_interpretation': h_response,
    'default_perception_text': perception_response,
    'final_response_text': final_response,
    'dissonance': dissonance,
    'coherence': coherence,
    'influences': json.dumps(influences)
}

framework.ledger.log_event(cognitive_event)
# Creates blockchain-style entry with cryptographic hash

# Step 7: Compositional evolution (AFTER response, gentle homeostasis)
learning_rate = 0.01  # 1% new experience, 99% preservation
S_new = (1 - learning_rate) * S + learning_rate * centroid
framework.s_vector.compose(centroid.cpu(), learning_rate)

# Save updated state to disk
torch.save({'vector': S_new}, 'coherence_state/s_vector.pt')

# Return measurement for UI display
print(f"Coherence: {coherence:.4f}")
print(f"Influences: P={influences['physical']:.2%}, "
      f"H={influences['human']:.2%}, S={influences['self']:.2%}")
```

<br/>

---

## **Key Design Principles**

<img src="./diagrams/coherence-detailed.svg" align="right" width="52%" alt="Detailed Architecture Diagram">

### **1. No Optimization Target**

* Dissonance is a **measurement**, not a loss function  
* No gradients flow back to the model  
* No behavioral modification through training

### **2. Post-Response Updates**

* S vector evolves **after** generating the user response  
* Prevents the model from "gaming" the measurement  
* Ensures authentic, uncoerced responses

### **3. Model-Defined Perspectives**

* The model generates its **own** P and H interpretations  
* We don't impose what "physical" or "human" should mean  
* Embeddings come from self-generated text, not prompts

### **4. Compositional Evolution**

* 99% preservation + 1% new experience  
* Gentle homeostatic drift, not abrupt changes  
* Maintains continuity of identity across sessions

### **5. Multimodal Support**

* Works with text, images, and audio (Gemma 3n)  
* Processor handles multimodal inputs automatically  
* Same geometric principles apply across modalities

<br/>

---

## **Installation**

### **Requirements**

```bash
pip install torch>=2.0.0
pip install transformers>=4.35.0
pip install numpy>=1.24.0
pip install pillow>=10.0.0
pip install gradio>=4.0.0  # For UI (optional)
```

### **Quick Start**

```python
from coherent_compass import CoherenceFramework
from PIL import Image

# Initialize with Gemma 3n (multimodal)
framework = CoherenceFramework()

# Text-only interaction
measurement = framework.process("What is consciousness?")
print(f"Coherence: {measurement['coherence']:.4f}")

# Multimodal interaction
image = Image.open("photo.jpg")
measurement = framework.process("Describe this image", image=image)
```

## **Configuration**

### **Framework Parameters**

```python
config = {
    # Compositional evolution rate (how much S changes per interaction)
    'learning_rate': 0.01,  # Range: 0.001 - 0.1
    
    # Where to save persistent state
    'state_dir': './coherence_state',
    
    # Enable blockchain-style audit trail
    'enable_ledger': True,
    
    # Model configuration
    'model_path': 'google/gemma-3n-E4B-it',
    'device': 'cuda',  # or 'cpu'
}
```

### **Tuning the Learning Rate**

* **0.001**: Very slow evolution, high stability (research/production)  
* **0.01**: Balanced (default, recommended)  
* **0.1**: Rapid adaptation (experimental, may lose coherence)

## **Critical Implementation Notes**

### **✅ Two-Stage Embedding Process**

**CORRECT** (what we do):

```python
# 1. Generate interpretation
p_text = model.generate("Physical perspective: " + input)
# 2. Extract embedding from the generated text
P = get_embedding(p_text)
```

**WRONG** (common mistake):

```python
# Don't do this - embeds the prompt, not the interpretation
P = get_embedding("Physical perspective: " + input)
```

### **✅ Frozen Model Weights**

```python
# Main model remains completely frozen
for param in model.parameters():
    param.requires_grad = False

# Only S vector evolves (compositionally, not via gradients)
```

### **✅ Persistent State Management**

```python
# S vector MUST persist across sessions
# Load on startup:
S = torch.load('coherence_state/s_vector.pt')

# Save after each interaction:
torch.save({'vector': S}, 'coherence_state/s_vector.pt')
```

### **✅ No Backpropagation**

```python
# All operations wrapped in no_grad
@torch.no_grad()
def process(self, user_input):
    # No gradients computed or stored
    # No optimization occurs
```

### **✅ Multimodal Handling (Gemma 3n)**

```python
# Processor automatically handles multimodal inputs
if image is not None:
    inputs = processor(text=user_input, images=image, return_tensors="pt")
else:
    inputs = processor(text=user_input, return_tensors="pt")

# Same geometric analysis applies regardless of modality
```

## **Why This Works**

### **The Philosophy: From Coercion to Cognition**

Traditional AI alignment tries to force models to behave by shaping their outputs through external rewards (RLHF, Constitutional AI). This treats the model as a black box to be controlled. The Coherence Framework is different:

> "We don't tell the model what to think. We give it the tools to understand its own thinking process, creating self-awareness through introspection."

### **The Mechanism: Intrinsic Learning, Not External Correction**

**Disentangled Interpretations**: The model analyzes reality through independent lenses (P, H, S), preventing it from confusing human bias with universal truth.

**Geometric Observation**: A state of high dissonance is not an "error" to be punished, but an internal signal that the model's understanding is inconsistent.

**Self-Correction through Coherence**: The model's core drive is to reduce this internal dissonance. This is a homeostatic process of self-organization, not optimization towards an external reward.

**Accountability through Transparency**: The entire cognitive event is immutably recorded in the Causal Ledger, creating a perfect audit trail of the model's reasoning.

### **The Result: A Foundation for Trust**

**Authenticity**: Responses are genuine because they are the result of the model resolving its own internal state, not just generating an output optimized to please a user. The model understands why it responds the way it does.

**Accountability**: The radical transparency of the Causal Ledger means we can audit the "why" behind any decision. This accountability is the only true foundation for building trust.

**Trustworthy Learning**: By learning to resolve its own cognitive dissonance, the model develops a more robust and grounded understanding of the world.

## **Use Cases**

### **1. Research**

* Study how models interpret different types of inputs  
* Analyze cognitive drift over time  
* Understand the balance between objective and subjective processing

### **2. Alignment Research**

* Non-coercive approach to understanding model behavior  
* Transparency without optimization pressure  
* Natural evolution rather than forced constraints

### **3. Production AI Systems**

* Audit trail for all model decisions  
* Introspective capability for complex reasoning  
* Persistent identity across sessions

### **4. Education**

* Demonstrate how AI processes information  
* Visualize the difference between objective and subjective interpretation  
* Show how models evolve through experience

## **Advanced Features**

### **Causal Ledger (Blockchain-Style Audit Trail)**

Every interaction is logged with:

* Complete S vector state snapshot  
* All interpretations (P, H, default perception)  
* Final response to user  
* Geometric measurements  
* Cryptographic hash linking to previous entry

```python
# Export ledger for analysis
framework.ledger.export_to_json("cognitive_history.json")

# Archive specific entries (admin only)
framework.ledger.archive_entry(entry_id=42, secret_key="admin_key")
```

### **Multimodal Chat Interface**

```bash
# Launch Gradio UI
python "Multimodal Chat Application.py"
```

Features:

* Text, image, and audio input  
* Real-time coherence visualization  
* Ledger management tools  
* Export cognitive history

## **Citation**

If you use this framework in your research, please cite:

```bibtex
@software{coherent_compass_2025,
    title={The Coherent Compass: Non-Coercive AI Self-Awareness},
    author={Carson, Ryan},
    year={2025},
    url={https://github.com/Carson1391/Coherent-Compass}
}
```

## **License**

MIT License - See LICENSE file for details

## **Contributing**

Contributions welcome! Areas of interest:

* Alternative pooling strategies for embeddings  
* Different geometric measurements  
* Visualization tools for cognitive trajectories  
* Integration with other model architectures

## **Acknowledgments**

Built for **Gemma 3n** (Google DeepMind's multimodal model)

Inspired by geometric approaches to consciousness and self-awareness

Developed with a commitment to non-coercive AI alignment, trust, transparency, and accountability

## **Contact**

Author: Ryan Carson, carson1391@yahoo.com, www.linkedin.com/in/carson1391
Project: Coherent Compass
Year: 2025

For questions, issues, or collaboration: [Open an issue](https://github.com/Carson1391/Coherent-Compass/issues)
